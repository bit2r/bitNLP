---
title: "Manipulate Documents"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Manipulate Documents}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r environment, echo = FALSE, message = FALSE, warning=FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "", out.width = "600px", dpi = 70,
                      echo = TRUE, message = FALSE, warning = FALSE)
options(tibble.print_min = 4L, tibble.print_max = 4L)
```

## Manipulate Documents

`Manipulate Documents`라 쓰고 텍스트 **`텍스트 데이터 정제`**라 이야기 합니다.

### 텍스트 데이터 정제

신문 기사나 소설, 수필과 같은 잘 정리된 텍스트 문서와 뉴스 진행자들이 전하는 뉴스 멘트들은 맞춤법에 부합하는 품질 높은 텍스트 데이터들입니다.

실제로 텍스트 분석에 직면하면 환상은 저 먼 나라의 이야기가 되어 버립니다. 맞춤법, 띄어쓰기가 무시된 텍스트는 그나마 애교가 있는 수준입니다.

통화 내용을 STT(Speech to Text)기법으로 텍스트로 변환한 데이터는 변환기의 성능이 완벽하지 않아서 품질이 매우 낮습니다. 화자와 청자의 의도는 유추하여 이해할 수 있겠으나, 텍스트 분석이라는 기계를 시켜서 수행하는 데이터 분석에는 부족함이 많습니다.

카페나 블로그의 게시글, SNS 채널의 글은 **맞춤법, 띄어쓰기에 취약하고, 신조어나 암호같은 줄임말, 완전하지 않은 문장들이 포함**됩니다. 경우에 따라서는 수집 과정에서 기술적인 한계로, **불필요한 텍스트들이 포함**되기도 합니다. 그래서 데이터 정제없이 분석할 수 없는 경우가 많습니다. 어떤 경우는 수집한 텍스트 데이터가 데이터 **분석을 수행하려는 목적과 부합하지 않아서 제거해야할** 경우도 있습니다.

이처럼 텍스트 데이터 분석은 일반적인 데이터 분석에 비해서 데이터 정제가 차지하는 비중은 매우 큽니다. 텍스트 데이터 정제 성능은 텍스트 데이터 분석 성능과 직결되기 때문입니다.

### 형태소분석과 데이터의 품질

텍스트 데이터 분석은 보통 형태소분석을 통해서 품사를 태깅하고, 품사 기반으로 토큰화된 단어로 텍스트 분석을 수행합니다.

문제는 분석에 사용하는 형태소분석기가, 문법과 띄어쓰기에 부합되는 품질 좋은 양질의 텍스트 데이터를 학습해서 만들어진 모델을 이용한다는 점입니다. 그래서 형태소분석을 수행하는 데이터의 품질이 떨어진다면, 형태소분석의 결과도 만족스럽지 못합니다. 어찌 보면 이러한 점이 데이터 정제를 하는 가장 큰 이유 중에 하나입니다.

문서의 품질이 높은 경우에도 문제가 발생할 수 있습니다. 형태소분석기에 사용한 학습 데이터는 우리가 일상 생활에서 이야기하는 대화의 주제, 혹은 직업, 학문과 예술, 종교 등 여러 분야의 내용을 모두 담지 못합니다. **학습 데이터는 지극히 일부의 샘플링된 문장들**이라는 점입니다. 그래서 통상적인 생활에서 발화되는 단어가 아닌 전문성이 필요한 영역의 단어를 이해하지 못합니다.

`알파고`가 쏘아 올린 화두가 학계와 필드의 AI 혁신을 이끌었습니다. 아마도 5년전에는 대중들은 `알파고`라는 단어에 익숙하지 못했을 겁니다.

이처럼 형태소분석기가 취약한 **신조어나, 특정 영역에서 사용하는 전문용어들은 사용자 정의 사전에 등록**해서 형태소분석기가 이를 이해할 수 있도록 도와줘야 합니다. 이러한 작업들도 광의적으로 데이터를 정제를 수행하는 덱트스 데이터의 조작(Manipulate Documents)입니다.

### 텍스트 데이터 정제를 위한 bitNLP의 기능

bitNLP의 텍스트 데이터 조작 기능을 정리하면 다음과 같습니다.

-   문서 단위의 전처리
    -   문서 필터링 (Filter Documents)
-   텍스트 단위의 전처리
    -   텍스트 대체 (Replace Texts)
    -   텍스트 연결 (Concatenate Texts)
    -   텍스트 분리 (Split Texts)
    -   텍스트 제거 (Remove Texts)

bitNLP는 대용량의 텍스트 데이터에서 상기 데이터 조작을 수행할 수 있도록 도와줍니다. 그래서 다음과 같은 방법으로 작업합니다.

-   병렬 처리를 통한 속도의 개선
-   데이터 조작 룰을 등록한 메타(meta) 파일 활용

## bitNLP의 메타 데이터 관리

### 메타 데이터 종류 및 데이터 포맷

| 메터 데이터 이름 | 메타 데이터 아이디 | 변수이름   | 변수 설명                                                       |
|------------------|--------------------|------------|-----------------------------------------------------------------|
| 문서 필터링      | filter             | rule_nm    | 개별 필터 룰의 이름                                             |
|                  |                    | pattern    | 문서 필터링을 위한 패턴 매치 정규표현식                         |
|                  |                    | accept     | allow/deny 여부, TRUE는 allow 패턴, FALSE는 deny 패턴           |
|                  |                    | use        | 개별 룰 사용여부, FALSE이면 미사용, TRUE인 건만 사용            |
| 텍스트 대체      | replace            | rule_nm    | 개별 대체 룰의 이름                                             |
|                  |                    | rule_class | 텍스트 대체 룰의 그룹 이름                                      |
|                  |                    | pattern    | 텍스트 대체를 위한 패턴 매치 정규표현식                         |
|                  |                    | replace    | 패턴에 매치된 텍스트를 대체할 텍스트 정의                       |
|                  |                    | use        | 개별 룰 사용여부, FALSE이면 미사용, TRUE인 건만 사용            |
| 텍스트 연결      | concat             | rule_nm    | 개별 연결 룰의 이름                                             |
|                  |                    | pattern    | 텍스트 연결을 위한 패턴 매치 정규표현식                         |
|                  |                    | replace    | 패턴에 매치된 텍스트를 대체할 텍스트 정의                       |
|                  |                    | use        | 개별 룰 사용여부, FALSE이면 미사용, TRUE인 건만 사용            |
| 텍스트 분리      | split              | rule_nm    | 개별 분리 룰의 이름                                             |
|                  |                    | pattern    | 텍스트 분리를 위한 패턴 매치 정규표현식                         |
|                  |                    | replace    | 패턴에 매치된 텍스트를 대체할 텍스트 정의                       |
|                  |                    | use        | 개별 룰 사용여부, FALSE이면 미사용, TRUE인 건만 사용            |
| 텍스트 제거      | remove             | rule_nm    | 개별 제거 필터의 이름                                           |
|                  |                    | pattern    | 텍스트 제거를 위한 패턴 매치 정규표현식                         |
|                  |                    | use        | 개별 룰 사용여부, FALSE이면 미사용, TRUE인 건만 사용            |

### 메타 데이터의 설정과 확인

`set_meta()` 함수는 세션 안에서 bitNLP 패키지의 메타 데이터를 등록합니다.

다음의 `set_meta()` 함수의 원형을 보면 데이터 파일을 읽는 방법과 유사합니다. 

````
set_meta(
  id = c("filter", "replace", "remove", "concat", "split"),
  filename,
  sep = ",",
  fileEncoding = "utf-8",
  append = FALSE
)
````

bitNLP 패키지는 샘플 메타 데이터 파일을 제공하는데, 문서 필터링을 위한 샘플 메타 데이터 파일을 읽어 봅니다. 

```{r}
library(bitNLP)

meta_path <- system.file("meta", package = "bitNLP")
fname <- glue::glue("{meta_path}/preparation_filter.csv")

## 데이터 필터링 메타 신규 등록
set_meta("filter", fname, fileEncoding = "utf8")
```

`get_meta()` 함수는 세션 안에서 등록된 메타 데이터를 조회합니다.

```{r}
## 기 등록된 데이터 필터링 메타 조회
get_meta("filter")
```

### filter_text()를 이용한 문서 필터링

텍스트 데이터(문서들) 중에서 분석을 수행하려는 목적과 부합하지 않은 텍스트(문서)를 제거해야할 경우에는 `filter_text()`를 사용합니다.

이미 앞에서 문서 필터링을 위한 메타 데이터 파일을 읽어들였습니다. 6개의 룰은 `accept` 값이 FALSE인 deny 룰입니다. 즉 해당 검색 패턴을 만족하는 텍스트 데이터를 제거하는 작업을 수행합니다.

#### 문자 벡터의 필터링

버즈 데이터의 본문은 길이가 1000인 문자 벡터입니다. 이 벡터는 5개의 결측치를 포함하고 있습니다.

```{r}
doc_content <- buzz$CONTENT
is.character(doc_content)
length(doc_content)

sum(is.na(doc_content))
```

8개의 코어를 이용해서 필터링을 수행합니다. `as_logical = FALSE`을 지정하면 문자 벡터의 필터링을 수행할 수 있습니다.

```{r, message=TRUE}
doc_after_character <- filter_text(doc_content, as_logical = FALSE, mc.cores = 8)

length(doc_after_character)
```

5개의 결측치와 6개의 룰에서 10개의 문서가 제거되어서 길이가 985인 문자 벡터가 만들어졌습니다.

#### 데이터 프레임의 필터링

tidytext 패키지를 이용해서 텍스트 데이터 분석을 수행한다면, 문자 벡터의 필터링이 아니라 문자 변수를 이용한 필터링을 수행해야 합니다.

다음처럼 `as_logical` 인수의 기본값인 TRUE를 사용합니다. 이 경우는 `CONTENT` 변수의 모든 원소에 대해서 allow 필터링 여부를 의미하는 논리 벡터를 만들어 반환합니다. 그러므로 `dplyr` 패키지의 `filter` 함수와 사용하여 필터링합니다.

```{r}
library(dplyr)

buzz %>% 
  filter(filter_text(CONTENT, verbos = FALSE)) %>% 
  select(KEYWORD, SRC, CONTENT)
```

### replace_text()를 이용한 텍스트 대체

문서 안에 포함된 특정 텍스트를 다른 텍스트로 대체하기 위해서는 `replace_text()`를 사용합니다. `as_logical` 인수만 없을 뿐 사용 방법은 `filter_text()`와 유사합니다.

```{r}
meta_path <- system.file("meta", package = "bitNLP")
fname <- glue::glue("{meta_path}/preparation_replace.csv")
set_meta("replace", fname, fileEncoding = "utf8")

# 등록된 문자열 대체 룰 확인하기
get_meta("replace")
```

`남편`이라는 단어와 `신랑`이라는 단어를 포함한 문장의 수는 각각 175개와 177개입니다. 그러나 이 두 단어는 동의어입니다. 그래서 텍스트 대체 룰에는 이 두 단어를 `남편`이라는 하나의 단어로 표준화했습니다.

```{r}
doc_content <- buzz$CONTENT

stringr::str_detect(doc_content, "남편") %>% 
  sum(na.rm = TRUE)

stringr::str_detect(doc_content, "신랑") %>% 
  sum(na.rm = TRUE)
```

문서들에서 몇 개의 룰이 적용되는지 결과를 보면서 텍스트를 대체합니다. `신랑`이라는 단어가 `남편`으로 대체되었음을 알 수 있습니다.

```{r, message=TRUE}
buzz_after <- buzz %>% 
  mutate(CONTENT = replace_text(CONTENT, verbos = TRUE))

stringr::str_detect(buzz_after$CONTENT, "남편") %>% 
  sum(na.rm = TRUE)

stringr::str_detect(buzz_after$CONTENT, "신랑") %>% 
  sum(na.rm = TRUE)
```
 

### concat_text()를 이용한 텍스트 연결

띄어쓰기된 단어들을 하나의 단어로 묶어주기 위해서 `concat_text()`를 사용합니다. 

```{r}
meta_path <- system.file("meta", package = "bitNLP")
fname <- glue::glue("{meta_path}/preparation_concat.csv")
set_meta("concat", fname, fileEncoding = "utf8")

# 등록된 문자열 결합 룰 확인하기
get_meta("concat")
```

일반적으로 복합명사를 정의하는 사례들입니다. 

`가사도우미`라는 단어는 `가사`와 `도우미`가 결합된 복합명사입니다. 그런데 두 단어가 띄어쓰기된 경우가 있습니다.

```{r}
doc_content <- buzz$CONTENT

stringr::str_detect(doc_content, "가사도우미") %>% 
  sum(na.rm = TRUE)

stringr::str_detect(doc_content, "가사[[:space:]]+도우미") %>% 
  sum(na.rm = TRUE)
```

문서들에서 몇 개의 룰이 적용되는지 결과를 보면서 텍스트를 연결합니다. 두 단어가 띄어쓰기된 `가사 도우미`가 수정되었습니다.

```{r, message=TRUE}
buzz_after <- buzz %>% 
  mutate(CONTENT = concat_text(CONTENT, verbos = TRUE))

stringr::str_detect(buzz_after$CONTENT, "가사도우미") %>% 
  sum(na.rm = TRUE)

stringr::str_detect(buzz_after$CONTENT, "가사[[:space:]]+도우미") %>% 
  sum(na.rm = TRUE)
``` 

이렇게 수정된 문서들이 형태소분석을 통해서 토큰화되어 분석을 수행한다면, 형태소분석기에도 복합명사가 등록되어 있어야 합니다. 안그러면 단어를 연경하여 복합명사를 만든어 놓아도 토큰화 과정에서 다시 분리됩니다.

다음처럼 mecab-ko의 사전에는 `가사도우미`라는 명사가 등록되어 있지 않습니다. 이 경우에는 사용자 정의 사전으로 토큰화 과정에서 다시 분리되지 않도록 유도해야 합니다.
 
```{r}
morpho_mecab("가사도우가 집안 청소를 했다.")
```


### split_text()를 이용한 텍스트 분리

묶어진 단어를 다시 분리할 경우에는 `split_text()`를 사용합니다. 

```{r}
meta_path <- system.file("meta", package = "bitNLP")
fname <- glue::glue("{meta_path}/preparation_split.csv")
set_meta("split", fname, fileEncoding = "utf8")

# 등록된 문자열 분리 룰 확인하기
get_meta("split")
```

`가사도우미`를 주제로 하는 것이 아니라 `도우미`를 주제로 분석하려 합니다. `도우미`가 들어간 복합명사를 분리해서 `도우미`라는 독립된 단어를 만들고자 합니다. `concat_text()`의 사례와는 반대의 경우입니다.

```{r}
doc_content <- buzz$CONTENT

stringr::str_extract_all(doc_content, "(하원|등하원|등원|입주|교포|가사|산후|보육|산모)(도우미)") %>% 
  unlist() %>% 
  na.omit() %>% 
  as.vector()
```

`도우미`가 들어간 복합명사들이 모두 분리되었습니다.

```{r, message=TRUE}
buzz_after <- buzz %>% 
  mutate(CONTENT = split_text(CONTENT, verbos = TRUE))

stringr::str_detect(buzz_after$CONTENT, "(하원|등하원|등원|입주|교포|가사|산후|보육|산모)(도우미)") %>% 
  sum(na.rm = TRUE)
``` 


### remove_text()를 이용한 텍스트 제거

문서 안에 불필요한 텍스트들이 포함되어 있을 수 있습니다. 그래서 문서 내에서 패턴 검색으로 불필요한 텍스트를 골라내어 제거할 수 있습니다.  `remove_text()`를 사용합니다. 

```{r}
meta_path <- system.file("meta", package = "bitNLP")
fname <- glue::glue("{meta_path}/preparation_remove.csv")
set_meta("remove", fname, fileEncoding = "utf8")

# 등록된 문자열 제거 룰 확인하기
get_meta("remove")
```

수집한 카페의 게시글에는 불필요한 텍스트들이 포함될 수 있습니다. 

다음은 카페의 게시글을 작성할 때, 관리자가 미리 설정해 놓은 주의사항을 삭제하지 않고 게시글을 작성한 문서들을 조회한 사례입니다. 그리고 불필요한 주의사항을 제거한 후의 내용은 어느 정도 정제가 되었습니다.  

```{r, tidy.opts = list(blank = FALSE, width.cutoff = 70)}
doc_content <- buzz$CONTENT

stringr::str_detect(doc_content, "게시판[[:space:]]*이용전[[:print:]]*이동됩니다.") %>% 
  which

doc_content[61]

stringr::str_remove(doc_content[61], "게시판[[:space:]]*이용전[[:print:]]*이동됩니다.") 

```

`remove_text()`로 불필요한 텍스트를 제거한 후에, 앞의 사례인 "게시판[[:space:]]\*이용전[[:print:]]\*이동됩니다."를 조회했습니다. 해당 문장이 삭제되어 패턴 검색이 되지 않았습니다. 

```{r, message=TRUE}
buzz_after <- buzz %>% 
  mutate(CONTENT = remove_text(CONTENT, verbos = TRUE))

stringr::str_detect(buzz_after$CONTENT, "게시판[[:space:]]*이용전[[:print:]]*이동됩니다.") %>% 
  sum(na.rm = TRUE)
``` 


